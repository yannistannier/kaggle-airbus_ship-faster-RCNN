{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.encodedPixel = pd.read_csv(\"train_ship_segmentations.csv\", header=0)\n",
    "        self.x_train = pd.read_csv(\"augmentation/train1.csv\", header=-1)[1]\n",
    "        self.x_test = pd.read_csv(\"augmentation/test1.csv\", header=-1)[1]\n",
    "        self.train_batch_count = self.x_train.shape[0] // self.batch_size\n",
    "        self.test_batch_count = self.x_test.shape[0] // 125\n",
    "        self.pp_mean = np.load(\"mean.npy\")\n",
    "        self.shuffle = np.random.permutation(self.x_train.shape[0])\n",
    "    \n",
    "    def normalize(self, batch_images):\n",
    "        return (batch_images - self.pp_mean) / 128.0\n",
    "    \n",
    "    def next_aug_train_batch(self, idx):\n",
    "        link_batch_images = self.x_train[self.shuffle[idx * self.batch_size: (idx + 1) * self.batch_size]]\n",
    "        batch_images = []\n",
    "        batch_mask = []\n",
    "        for x in link_batch_images:\n",
    "            img = cv2.imread('all/train/'+x)\n",
    "            batch_images.append(img)\n",
    "            \n",
    "            rle = self.encodedPixel.query('ImageId==\"'+x+'\"')['EncodedPixels'].tolist()\n",
    "            mask = self.masks_as_image(rle)\n",
    "            batch_mask.append(mask)\n",
    "        \n",
    "        return self.normalize(np.array(batch_images)), np.array(batch_mask)\n",
    "    \n",
    "    \n",
    "    def masks_as_image(self, in_mask_list, all_masks=None):\n",
    "        if all_masks is None:\n",
    "            all_masks = np.zeros((768, 768), dtype = np.int16)\n",
    "        #if isinstance(in_mask_list, list):\n",
    "        for mask in in_mask_list:\n",
    "            if isinstance(mask, str):\n",
    "                all_masks += self.rle_decode(mask)\n",
    "        return np.expand_dims(all_masks, -1)\n",
    "\n",
    "    def rle_decode(self, mask_rle, shape=(768, 768)):\n",
    "        s = mask_rle.split()\n",
    "        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "        starts -= 1\n",
    "        ends = starts + lengths\n",
    "        img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "        for lo, hi in zip(starts, ends):\n",
    "            img[lo:hi] = 1\n",
    "        return img.reshape(shape).T  # Needed to align to RLE direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_augmentation(image, mask):\n",
    "    \"\"\"Returns (maybe) augmented images\n",
    "    (1) Random flip (left <--> right)\n",
    "    (2) Random flip (up <--> down)\n",
    "    (3) Random brightness\n",
    "    (4) Random hue\n",
    "    Args:\n",
    "        image (3-D Tensor): Image tensor of (H, W, C)\n",
    "        mask (3-D Tensor): Mask image tensor of (H, W, 1)\n",
    "    Returns:\n",
    "        image: Maybe augmented image (same shape as input `image`)\n",
    "        mask: Maybe augmented mask (same shape as input `mask`)\n",
    "    \"\"\"\n",
    "    concat_image = tf.concat([image, mask], axis=-1)\n",
    "\n",
    "    maybe_flipped = tf.image.random_flip_left_right(concat_image)\n",
    "    maybe_flipped = tf.image.random_flip_up_down(concat_image)\n",
    "\n",
    "    image = maybe_flipped[:, :, :-1]\n",
    "    mask = maybe_flipped[:, :, -1:]\n",
    "\n",
    "    image = tf.image.random_brightness(image, 0.7)\n",
    "    image = tf.image.random_hue(image, 0.3)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def get_image_mask(queue, augmentation=False):\n",
    "    \"\"\"Returns `image` and `mask`\n",
    "    Input pipeline:\n",
    "        Queue -> CSV -> FileRead -> Decode JPEG\n",
    "    (1) Queue contains a CSV filename\n",
    "    (2) Text Reader opens the CSV\n",
    "        CSV file contains two columns\n",
    "        [\"path/to/image.jpg\", \"path/to/mask.jpg\"]\n",
    "    (3) File Reader opens both files\n",
    "    (4) Decode JPEG to tensors\n",
    "    Notes:\n",
    "        height, width = 640, 960\n",
    "    Returns\n",
    "        image (3-D Tensor): (640, 960, 3)\n",
    "        mask (3-D Tensor): (640, 960, 1)\n",
    "    \"\"\"\n",
    "    text_reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    _, csv_content = text_reader.read(queue)\n",
    "\n",
    "    image_path, mask_path = tf.decode_csv(\n",
    "        csv_content, record_defaults=[[\"\"], [\"\"]])\n",
    "\n",
    "    image_file = tf.read_file(image_path)\n",
    "    mask_file = tf.read_file(mask_path)\n",
    "\n",
    "    image = tf.image.decode_jpeg(image_file, channels=3)\n",
    "    image.set_shape([768, 768, 3])\n",
    "    image = tf.cast(image, tf.float32)\n",
    "\n",
    "    mask = tf.image.decode_jpeg(mask_file, channels=1)\n",
    "    mask.set_shape([768, 768, 1])\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    mask = mask / (tf.reduce_max(mask) + 1e-7)\n",
    "\n",
    "    if augmentation:\n",
    "        image, mask = image_augmentation(image, mask)\n",
    "\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_conv_pool(input_,\n",
    "                   n_filters,\n",
    "                   training,\n",
    "                   flags,\n",
    "                   name,\n",
    "                   pool=True,\n",
    "                   activation=tf.nn.relu):\n",
    "    \"\"\"{Conv -> BN -> RELU}x2 -> {Pool, optional}\n",
    "    Args:\n",
    "        input_ (4-D Tensor): (batch_size, H, W, C)\n",
    "        n_filters (list): number of filters [int, int]\n",
    "        training (1-D Tensor): Boolean Tensor\n",
    "        name (str): name postfix\n",
    "        pool (bool): If True, MaxPool2D\n",
    "        activation: Activaion functions\n",
    "    Returns:\n",
    "        net: output of the Convolution operations\n",
    "        pool (optional): output of the max pooling operations\n",
    "    \"\"\"\n",
    "    net = input_\n",
    "\n",
    "    with tf.variable_scope(\"layer{}\".format(name)):\n",
    "        for i, F in enumerate(n_filters):\n",
    "            net = tf.layers.conv2d(\n",
    "                net,\n",
    "                F, (3, 3),\n",
    "                activation=None,\n",
    "                padding='same',\n",
    "                kernel_regularizer=tf.contrib.layers.l2_regularizer(flags[\"reg\"]),\n",
    "                name=\"conv_{}\".format(i + 1))\n",
    "            net = tf.layers.batch_normalization(\n",
    "                net, training=training, name=\"bn_{}\".format(i + 1))\n",
    "            net = activation(net, name=\"relu{}_{}\".format(name, i + 1))\n",
    "\n",
    "        if pool is False:\n",
    "            return net\n",
    "\n",
    "        pool = tf.layers.max_pooling2d(\n",
    "            net, (2, 2), strides=(2, 2), name=\"pool_{}\".format(name))\n",
    "\n",
    "        return net, pool\n",
    "\n",
    "\n",
    "def upconv_concat(inputA, input_B, n_filter, flags, name):\n",
    "    \"\"\"Upsample `inputA` and concat with `input_B`\n",
    "    Args:\n",
    "        input_A (4-D Tensor): (N, H, W, C)\n",
    "        input_B (4-D Tensor): (N, 2*H, 2*H, C2)\n",
    "        name (str): name of the concat operation\n",
    "    Returns:\n",
    "        output (4-D Tensor): (N, 2*H, 2*W, C + C2)\n",
    "    \"\"\"\n",
    "    up_conv = upconv_2D(inputA, n_filter, flags, name)\n",
    "\n",
    "    return tf.concat(\n",
    "        [up_conv, input_B], axis=-1, name=\"concat_{}\".format(name))\n",
    "\n",
    "\n",
    "def upconv_2D(tensor, n_filter, flags, name):\n",
    "    \"\"\"Up Convolution `tensor` by 2 times\n",
    "    Args:\n",
    "        tensor (4-D Tensor): (N, H, W, C)\n",
    "        n_filter (int): Filter Size\n",
    "        name (str): name of upsampling operations\n",
    "    Returns:\n",
    "        output (4-D Tensor): (N, 2 * H, 2 * W, C)\n",
    "    \"\"\"\n",
    "\n",
    "    return tf.layers.conv2d_transpose(\n",
    "        tensor,\n",
    "        filters=n_filter,\n",
    "        kernel_size=2,\n",
    "        strides=2,\n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(flags[\"reg\"]),\n",
    "        name=\"upsample_{}\".format(name))\n",
    "\n",
    "\n",
    "def make_unet(X, training, flags=None):\n",
    "    \"\"\"Build a U-Net architecture\n",
    "    Args:\n",
    "        X (4-D Tensor): (N, H, W, C)\n",
    "        training (1-D Tensor): Boolean Tensor is required for batchnormalization layers\n",
    "    Returns:\n",
    "        output (4-D Tensor): (N, H, W, C)\n",
    "            Same shape as the `input` tensor\n",
    "    Notes:\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "    \"\"\"\n",
    "    net = X / 127.5 - 1\n",
    "    conv1, pool1 = conv_conv_pool(net, [8, 8], training, flags, name=1)\n",
    "    conv2, pool2 = conv_conv_pool(pool1, [16, 16], training, flags, name=2)\n",
    "    conv3, pool3 = conv_conv_pool(pool2, [32, 32], training, flags, name=3)\n",
    "    conv4, pool4 = conv_conv_pool(pool3, [64, 64], training, flags, name=4)\n",
    "    conv5 = conv_conv_pool(\n",
    "        pool4, [128, 128], training, flags, name=5, pool=False)\n",
    "\n",
    "    up6 = upconv_concat(conv5, conv4, 64, flags, name=6)\n",
    "    conv6 = conv_conv_pool(up6, [64, 64], training, flags, name=6, pool=False)\n",
    "\n",
    "    up7 = upconv_concat(conv6, conv3, 32, flags, name=7)\n",
    "    conv7 = conv_conv_pool(up7, [32, 32], training, flags, name=7, pool=False)\n",
    "\n",
    "    up8 = upconv_concat(conv7, conv2, 16, flags, name=8)\n",
    "    conv8 = conv_conv_pool(up8, [16, 16], training, flags, name=8, pool=False)\n",
    "\n",
    "    up9 = upconv_concat(conv8, conv1, 8, flags, name=9)\n",
    "    conv9 = conv_conv_pool(up9, [8, 8], training, flags, name=9, pool=False)\n",
    "\n",
    "    return tf.layers.conv2d(\n",
    "        conv9,\n",
    "        1, (1, 1),\n",
    "        name='final',\n",
    "        activation=tf.nn.sigmoid,\n",
    "        padding='same')\n",
    "\n",
    "\n",
    "def IOU_(y_pred, y_true):\n",
    "    \"\"\"Returns a (approx) IOU score\n",
    "    intesection = y_pred.flatten() * y_true.flatten()\n",
    "    Then, IOU = 2 * intersection / (y_pred.sum() + y_true.sum() + 1e-7) + 1e-7\n",
    "    Args:\n",
    "        y_pred (4-D array): (N, H, W, 1)\n",
    "        y_true (4-D array): (N, H, W, 1)\n",
    "    Returns:\n",
    "        float: IOU score\n",
    "    \"\"\"\n",
    "    H, W, _ = y_pred.get_shape().as_list()[1:]\n",
    "\n",
    "    pred_flat = tf.reshape(y_pred, [-1, H * W])\n",
    "    true_flat = tf.reshape(y_true, [-1, H * W])\n",
    "\n",
    "    intersection = 2 * tf.reduce_sum(pred_flat * true_flat, axis=1) + 1e-7\n",
    "    denominator = tf.reduce_sum(\n",
    "        pred_flat, axis=1) + tf.reduce_sum(\n",
    "            true_flat, axis=1) + 1e-7\n",
    "\n",
    "    return tf.reduce_mean(intersection / denominator)\n",
    "\n",
    "\n",
    "def make_train_op(y_pred, y_true):\n",
    "    \"\"\"Returns a training operation\n",
    "    Loss function = - IOU(y_pred, y_true)\n",
    "    IOU is\n",
    "        (the area of intersection)\n",
    "        --------------------------\n",
    "        (the area of two boxes)\n",
    "    Args:\n",
    "        y_pred (4-D Tensor): (N, H, W, 1)\n",
    "        y_true (4-D Tensor): (N, H, W, 1)\n",
    "    Returns:\n",
    "        train_op: minimize operation\n",
    "    \"\"\"\n",
    "    loss = -IOU_(y_pred, y_true)\n",
    "\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    optim = tf.train.AdamOptimizer()\n",
    "    return optim.minimize(loss, global_step=global_step)\n",
    "\n",
    "\n",
    "def read_flags():\n",
    "    \"\"\"Returns flags\"\"\"\n",
    "    \n",
    "    return dict({\n",
    "        \"epochs\":1,\n",
    "        \"batch-size\":10,\n",
    "        \"logdir\":\"logdir\",\n",
    "        \"reg\":0.1,\n",
    "        \"ckdir\":\"models\"\n",
    "    })\n",
    "    \n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\n",
    "        \"--epochs\", default=1, type=int, help=\"Number of epochs\")\n",
    "\n",
    "    parser.add_argument(\"--batch-size\", default=4, type=int, help=\"Batch size\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--logdir\", default=\"logdir\", help=\"Tensorboard log directory\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--reg\", type=float, default=0.1, help=\"L2 Regularizer Term\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--ckdir\", default=\"models\", help=\"Checkpoint directory\")\n",
    "\n",
    "    flags = parser.parse_args()\n",
    "    return flags\n",
    "\n",
    "\n",
    "def main(flags):\n",
    "    train = pd.read_csv(\"augmentation/train1.csv\")\n",
    "    n_train = train.shape[0]\n",
    "\n",
    "    test = pd.read_csv(\"augmentation/test1.csv\")\n",
    "    n_test = test.shape[0]\n",
    "\n",
    "    current_time = time.strftime(\"%m/%d/%H/%M/%S\")\n",
    "    train_logdir = os.path.join(\"model/\", \"train\", current_time)\n",
    "    test_logdir = os.path.join(\"model/\", \"test\", current_time)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, shape=[None, 768, 768, 3], name=\"X\")\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 768, 768, 1], name=\"y\")\n",
    "    mode = tf.placeholder(tf.bool, name=\"mode\")\n",
    "\n",
    "    pred = make_unet(X, mode, flags)\n",
    "\n",
    "    tf.add_to_collection(\"inputs\", X)\n",
    "    tf.add_to_collection(\"inputs\", mode)\n",
    "    tf.add_to_collection(\"outputs\", pred)\n",
    "\n",
    "    tf.summary.histogram(\"Predicted_Mask\", pred)\n",
    "    tf.summary.image(\"Predicted_Mask\", pred)\n",
    "\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_op = make_train_op(pred, y)\n",
    "\n",
    "    IOU_op = IOU_(pred, y)\n",
    "    IOU_op = tf.Print(IOU_op, [IOU_op])\n",
    "    tf.summary.scalar(\"IOU\", IOU_op)\n",
    "\n",
    "#     train_csv = tf.train.string_input_producer(['train.csv'])\n",
    "#     test_csv = tf.train.string_input_producer(['test.csv'])\n",
    "    \n",
    "#     train_image, train_mask = get_image_mask(train_csv)\n",
    "#     test_image, test_mask = get_image_mask(test_csv, augmentation=False)\n",
    "    \n",
    "#     print(train_image)\n",
    "    \n",
    "#     return None\n",
    "    \n",
    "#     X_batch_op, y_batch_op = tf.train.shuffle_batch(\n",
    "#         [train_image, train_mask],\n",
    "#         batch_size=flags[\"batch_size\"],\n",
    "#         capacity=flags[\"batch_size\"] * 5,\n",
    "#         min_after_dequeue=flags[\"batch_size\"] * 2,\n",
    "#         allow_smaller_final_batch=True)\n",
    "\n",
    "#     X_test_op, y_test_op = tf.train.batch(\n",
    "#         [test_image, test_mask],\n",
    "#         batch_size=flags[\"batch_size\"],\n",
    "#         capacity=flags[\"batch_size\"] * 2,\n",
    "#         allow_smaller_final_batch=True)\n",
    "\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    dataset = Dataset(128)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        train_summary_writer = tf.summary.FileWriter(train_logdir, sess.graph)\n",
    "        test_summary_writer = tf.summary.FileWriter(test_logdir)\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        if os.path.exists(flags[\"ckdir\"]) and tf.train.checkpoint_exists(flags[\"ckdir\"]):\n",
    "            latest_check_point = tf.train.latest_checkpoint(flags[\"ckdir\"])\n",
    "            saver.restore(sess, latest_check_point)\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                os.rmdir(flags[\"ckdir\"])\n",
    "            except FileNotFoundError:\n",
    "                pass\n",
    "            os.mkdir(flags[\"ckdir\"])\n",
    "\n",
    "        try:\n",
    "            global_step = tf.train.get_global_step(sess.graph)\n",
    "\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "            for epoch in range(flags[\"epochs\"]):\n",
    "\n",
    "                for step in range(dataset.train_batch_count):\n",
    "\n",
    "                    X_batch, y_batch = dataset.next_aug_train_batch(step)\n",
    "\n",
    "                    _, step_iou, step_summary, global_step_value = sess.run(\n",
    "                        [train_op, IOU_op, summary_op, global_step],\n",
    "                        feed_dict={X: X_batch,\n",
    "                                   y: y_batch,\n",
    "                                   mode: True})\n",
    "\n",
    "                    train_summary_writer.add_summary(step_summary,\n",
    "                                                     global_step_value)\n",
    "\n",
    "                total_iou = 0\n",
    "                for step in range(0, n_test, flags[\"batch_size\"]):\n",
    "                    X_test, y_test = sess.run([X_test_op, y_test_op])\n",
    "                    step_iou, step_summary = sess.run(\n",
    "                        [IOU_op, summary_op],\n",
    "                        feed_dict={X: X_test,\n",
    "                                   y: y_test,\n",
    "                                   mode: False})\n",
    "\n",
    "                    total_iou += step_iou * X_test.shape[0]\n",
    "\n",
    "                    test_summary_writer.add_summary(step_summary,\n",
    "                                                    (epoch + 1) * (step + 1))\n",
    "\n",
    "            saver.save(sess, \"{}/model.ckpt\".format(flags[\"ckdir\"]))\n",
    "\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            saver.save(sess, \"{}/model.ckpt\".format(flags[\"ckdir\"]))\n",
    "\n",
    "            \n",
    "flags = read_flags()\n",
    "main(flags)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
